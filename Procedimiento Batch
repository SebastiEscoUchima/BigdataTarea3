# tarea3_batch.py  (continuación / reemplazo lectura y procesamiento)
# -----------------------------------------------------------
# Objetivo: leer robustamente, limpiar, transformar, EDA y guardar resultados.
# -----------------------------------------------------------

from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import IntegerType, DoubleType, TimestampType, DateType
import sys

spark = SparkSession.builder \
    .appName("Tarea3_Batch_CasosCOVID") \
    .getOrCreate()

# Ruta HDFS del CSV original
file_path = "hdfs://localhost:9000/Tarea3/gt2j-8ykr.csv"

# -------------------------
# 1) Lectura robusta del CSV
# Intentamos con coma (formato CSV estándar). Si el resultado tiene 1 columna
# (indicando que el separador fue incorrecto), mostramos aviso y tratamos de limpiar.
# -------------------------
df_try = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .option("sep", ",") \
    .option("quote", "\"") \
    .option("escape", "\\") \
    .option("multiLine", "false") \
    .option("nullValue", "") \
    .load(file_path)

# Si se cargó todo en una sola columna (len == 1) — reintentamos con separación por pipe
if len(df_try.columns) == 1:
    print("Lectura inicial produjo una sola columna. Reintentando con separador '|' y opciones de quote.")
    df_raw = spark.read.format("csv") \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .option("sep", "|") \
        .option("quote", "\"") \
        .option("escape", "\\") \
        .option("multiLine", "false") \
        .option("nullValue", "") \
        .load(file_path)
else:
    df_raw = df_try

# Mostrar esquema / primeras filas para inspección
print("=== Esquema detectado ===")
df_raw.printSchema()
print("=== Muestra de filas ===")
df_raw.show(10, truncate=False)

# -------------------------
# 2) Normalizar nombres de columnas (quitar comillas, espacios, caracteres raros)
# -------------------------
def clean_col_name(c):
    # elimina comillas dobles y espacios extremos y cambia espacios por guion bajo
    return c.strip().replace('"', '').replace(' ', '_')

new_cols = [clean_col_name(c) for c in df_raw.columns]
df = df_raw.toDF(*new_cols)

print("=== Columnas limpias ===")
print(df.columns)

# -------------------------
# 3) Transformaciones de tipo (fechas y numéricos)
# Columnas de fecha detectadas (ajusta si los nombres difieren)
# -------------------------
# Lista de columnas de fecha esperadas en tu dataset original
date_cols = [
    "fecha_reporte_web",
    "fecha_de_notificaci_n",
    "fecha_inicio_sintomas",
    "fecha_muerte",
    "fecha_diagnostico",
    "fecha_recuperado"
]

# Formato de fecha en tus muestras: "2020-12-24 00:00:00"
for c in date_cols:
    if c in df.columns:
        # Convertir a Timestamp (mantiene hora) y luego opcionalmente a Date si deseas sólo fecha
        df = df.withColumn(c + "_ts", F.to_timestamp(F.col(c), "yyyy-MM-dd HH:mm:ss")) \
               .withColumn(c + "_date", F.to_date(F.col(c + "_ts")))
        # (opcional) borrar columna original string si lo deseas: .drop(c)
    else:
        print(f"Aviso: columna de fecha esperada no encontrada: {c}")

# Convertir edad y unidad_medida a enteros si existen
if "edad" in df.columns:
    df = df.withColumn("edad_int", F.col("edad").cast(IntegerType()))
if "unidad_medida" in df.columns:
    df = df.withColumn("unidad_medida_int", F.col("unidad_medida").cast(IntegerType()))

# -------------------------
# 4) Limpieza de valores categóricos y nulos
# - Normalizar sexo (mayúscula)
# - Limpiar espacios en departamento, ciudad
# - Reemplazar cadenas vacías por null
# -------------------------
# trim a varias columnas de texto
text_cols = [c for c, t in df.dtypes if t.startswith("string")]
for c in text_cols:
    df = df.withColumn(c, F.when(F.trim(F.col(c)) == "", None).otherwise(F.trim(F.col(c))))

# Normalizar sexo
if "sexo" in df.columns:
    df = df.withColumn("sexo", F.upper(F.col("sexo")))

# Manejo de columna 'recuperado' o 'estado' si existen — unificar posibles variantes
if "recuperado" in df.columns:
    df = df.withColumn("recuperado_norm", F.when(F.col("recuperado").isin("Recuperado", "recuperado", "RECUPERADO", "Si", "S"), "Si")
                                            .when(F.col("recuperado").isNull(), None)
                                            .otherwise(F.col("recuperado")))

# -------------------------
# 5) Columnas finales: seleccionar/renombrar las columnas útiles
# Ajusta la lista según lo que quieras guardar
# -------------------------
selected_cols = [
    "id_de_caso" if "id_de_caso" in df.columns else "id_de_caso",
    "departamento_nom" if "departamento_nom" in df.columns else "departamento",
    "ciudad_municipio_nom" if "ciudad_municipio_nom" in df.columns else "ciudad_municipio",
    "edad_int",
    "unidad_medida_int",
    "sexo",
    "fuente_tipo_contagio" if "fuente_tipo_contagio" in df.columns else "fuente_tipo_contagio",
    "ubicacion" if "ubicacion" in df.columns else "ubicacion",
    "estado" if "estado" in df.columns else "estado",
    "recuperado_norm",
]

# Filtrar selected real (solo las que existen)
selected_cols = [c for c in selected_cols if c in df.columns]

# añadir algunas fechas parseadas si existen
for c in date_cols:
    c_date = c + "_date"
    if c_date in df.columns:
        selected_cols.append(c_date)

df_processed = df.select(*selected_cols).withColumnRenamed("departamento_nom", "departamento") \
                                           .withColumnRenamed("ciudad_municipio_nom", "ciudad")

print("=== Esquema final procesado ===")
df_processed.printSchema()
print("=== Muestra procesada ===")
df_processed.show(10, truncate=False)

# -------------------------
# 6) Análisis exploratorio rápido (EDA)
# Guardamos algunas métricas/aggregados útiles.
# -------------------------
# 6.1 Conteo por departamento
if "departamento" in df_processed.columns:
    dept_counts = df_processed.groupBy("departamento").count().orderBy(F.desc("count"))
    print("=== Top 20 departamentos por número de casos ===")
    dept_counts.show(20, truncate=False)
else:
    dept_counts = None
    print("Columna 'departamento' no encontrada para EDA.")

# 6.2 Conteo por sexo
if "sexo" in df_processed.columns:
    sexo_counts = df_processed.groupBy("sexo").count().orderBy(F.desc("count"))
    print("=== Conteo por sexo ===")
    sexo_counts.show(truncate=False)
else:
    sexo_counts = None

# 6.3 Conteo por estado (p.ej. 'Leve','Fallecido','Grave', etc.)
if "estado" in df_processed.columns:
    estado_counts = df_processed.groupBy("estado").count().orderBy(F.desc("count"))
    print("=== Conteo por estado ===")
    estado_counts.show(truncate=False)
else:
    estado_counts = None

# 6.4 Estadísticas de edad
if "edad_int" in df_processed.columns:
    edad_stats = df_processed.select(
        F.count("edad_int").alias("n"),
        F.mean("edad_int").alias("edad_media"),
        F.stddev("edad_int").alias("edad_std"),
        F.min("edad_int").alias("edad_min"),
        F.max("edad_int").alias("edad_max")
    )
    print("=== Estadísticas de edad ===")
    edad_stats.show(truncate=False)
else:
    edad_stats = None

# -------------------------
# 7) Guardar resultados procesados en HDFS
# - Data procesada -> Parquet (eficiente para Spark)
# - Resúmenes EDA -> CSV para consulta rápida
# -------------------------
out_base = "hdfs://localhost:9000/Tarea3/processed"

# Guardar parquet (sobrescribe si existe)
df_processed.write.mode("overwrite").parquet(out_base + "/parquet")

# Guardar agregados EDA si existen
if dept_counts is not None:
    dept_counts.coalesce(1).write.mode("overwrite").option("header", "true").csv(out_base + "/eda/departamento_counts")
if sexo_counts is not None:
    sexo_counts.coalesce(1).write.mode("overwrite").option("header", "true").csv(out_base + "/eda/sexo_counts")
if estado_counts is not None:
    estado_counts.coalesce(1).write.mode("overwrite").option("header", "true").csv(out_base + "/eda/estado_counts")
if edad_stats is not None:
    edad_stats.coalesce(1).write.mode("overwrite").option("header", "true").csv(out_base + "/eda/edad_stats")

print("Guardado completado en HDFS en:", out_base)

# Fin de script
spark.stop()
